{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c06a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# طريقة تنزيل الداتا سيت لتعمل مع المشروع\n",
    "# Kaggle API Key setup\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the Kaggle API key file\n",
    "uploaded = files.upload()\n",
    "\n",
    "!pip install kaggle\n",
    "\n",
    "# Move the Kaggle API key to the correct directory\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download the MNIST dataset\n",
    "!kaggle datasets download -d oddrationale/mnist-in-csv\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip mnist-in-csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3018f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import fashion_mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58af2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a5d7c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MNIST] Caching data at FASHION_MNIST\n",
      "[MNIST] Found http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz in cache.\n",
      "[MNIST] Found http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz in cache.\n",
      "[MNIST] Found http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz in cache.\n",
      "[MNIST] Found http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz in cache.\n"
     ]
    }
   ],
   "source": [
    "x,y,x_test,y_test = fashion_mnist('FASHION_MNIST')\n",
    "y_class = y \n",
    "t_class_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acb183a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(x.shape[0],784)\n",
    "x_test = x_test.reshape(x_test.shape[0],784)\n",
    "x = x / 255.0\n",
    "x_test = x_test / 255.0\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d4935d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the training and test labels to one-hot vectors\n",
    "y = ohe.fit_transform(y.reshape(-1, 1))\n",
    "y_test = ohe.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f47d6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code of the resilient algorithm \n",
    "class NN:\n",
    "    #initialize new object of nueral network \n",
    "    def __init__(self, X,Y, L=1,N_l=256,epochs=10, lr=0.001):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.layer_sizes =np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.__init_weights()\n",
    "        self.__init_pre_gradient()\n",
    "        self.__init_learning_rate()\n",
    "        self.__init_pre_change_rate()\n",
    "        self.__init_gradient()\n",
    "        self.__init_change_rate()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.train_time,self.tot_time]\n",
    "        shuffle = np.random.permutation(self.n_samples)\n",
    "        self.X , self.X_val = np.split(self.X,[int(0.8*self.n_samples)])\n",
    "        self.Y , self.Y_val = np.split(self.Y,[int(0.8*self.n_samples)])\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "\n",
    "    def _positive_sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def _negative_sigmoid(self,x):\n",
    "        # Cache exp so you won't have to calculate it twice\n",
    "        exp = np.exp(x)\n",
    "        return exp / (exp + 1)\n",
    "    \n",
    "    \n",
    "    def __sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return self.sigmoid(x,False)*(1 - self.sigmoid(x,False))\n",
    "        positive = x >= 0\n",
    "        # Boolean array inversion is faster than another comparison\n",
    "        negative = ~positive\n",
    "\n",
    "        # empty contains juke hence will be faster to allocate than zeros\n",
    "        result = np.empty_like(x)\n",
    "        result[positive] = self._positive_sigmoid(x[positive])\n",
    "        result[negative] = self._negative_sigmoid(x[negative])\n",
    "\n",
    "        return result\n",
    "    def __softmax(self,x):\n",
    "        # Compute softmax along the rows of the input   \n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "\n",
    "    # cross entropy loss function\n",
    "    def __loss(self,y_pred,y):\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "\n",
    "    # calculate the accurac\n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "\n",
    "    def __sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "\n",
    "\n",
    "    def __init_gradient(self):\n",
    "        self.gradient = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.gradient.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
    "\n",
    "    def __init_pre_gradient(self):\n",
    "        self.pre_gradient = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.pre_gradient.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
    "            \n",
    "    def __init_pre_change_rate(self):\n",
    "        self.pre_change_rate = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.pre_change_rate.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))  \n",
    "            \n",
    "    def __init_change_rate(self):\n",
    "        self.change_rate = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.change_rate.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))  \n",
    "   \n",
    "    def __init_learning_rate(self):\n",
    "        self.learning_rate = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.learning_rate.append(np.full((self.layer_sizes[i],self.layer_sizes[i+1]),0.01))\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations \n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "        \n",
    "    def __feed_forward(self,batch):\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__sigmoid(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    #the function that implements resilient algorithm \n",
    "    def resilient(self,prev_gradient,gradient,learning_rate,pre_change_rate) :         \n",
    "        gradient_multiplication = gradient * prev_gradient\n",
    "        learning_mult = np.where(gradient_multiplication < 0, 0.5, np.where(gradient_multiplication == 0, 1, 1.2))\n",
    "        learning_rate = np.clip(learning_rate * learning_mult,1e-12,50)\n",
    "        curr_change_rate = learning_rate * np.sign(gradient)\n",
    "        np.copyto(curr_change_rate,pre_change_rate * -1, where=(gradient_multiplication < 0))\n",
    "        prev_gradient = np.where(gradient_multiplication < 0, 0, gradient)\n",
    "        \n",
    "        return curr_change_rate\n",
    "    \n",
    "    # function for calculate backpropagation\n",
    "    def __back_prop(self,batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        delta_t = (self.__out - batch_y)*self.__sigmoid_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.gradient[-i] = (self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            self.change_rate[-i] = self.resilient(self.pre_gradient[-i],self.gradient[-i],self.learning_rate[-i],self.pre_change_rate[-i])\n",
    "            delta_t = self.__sigmoid_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.change_rate[-i]\n",
    "            \n",
    "    def predict(self,X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        X = X\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    # make an evaluation for the test data \n",
    "    def evaluate(self,X,Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)         \n",
    "            \n",
    "    def __to_categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "\n",
    "    # responsible for training the model\n",
    "    def train(self,batch_size=16,epochs=25,lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} |val_acc = {val_acc.round(3)}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acfc252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NN(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f328be5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 1.696 | acc = 0.695 | val_loss = 1.6 |val_acc = 0.798\n",
      "Epoch 2: loss = 1.593 | acc = 0.817 | val_loss = 1.597 |val_acc = 0.821\n",
      "Epoch 3: loss = 1.584 | acc = 0.833 | val_loss = 1.594 |val_acc = 0.832\n",
      "Epoch 4: loss = 1.58 | acc = 0.841 | val_loss = 1.585 |val_acc = 0.842\n",
      "Epoch 5: loss = 1.58 | acc = 0.846 | val_loss = 1.58 |val_acc = 0.843\n",
      "Epoch 6: loss = 1.581 | acc = 0.847 | val_loss = 1.579 |val_acc = 0.848\n",
      "Epoch 7: loss = 1.579 | acc = 0.85 | val_loss = 1.594 |val_acc = 0.842\n",
      "Epoch 8: loss = 1.58 | acc = 0.85 | val_loss = 1.6 |val_acc = 0.848\n",
      "Epoch 9: loss = 1.582 | acc = 0.852 | val_loss = 1.586 |val_acc = 0.849\n",
      "Epoch 10: loss = 1.584 | acc = 0.853 | val_loss = 1.586 |val_acc = 0.852\n",
      "Epoch 11: loss = 1.585 | acc = 0.855 | val_loss = 1.593 |val_acc = 0.847\n",
      "Epoch 12: loss = 1.588 | acc = 0.852 | val_loss = 1.594 |val_acc = 0.853\n",
      "Epoch 13: loss = 1.591 | acc = 0.853 | val_loss = 1.586 |val_acc = 0.852\n",
      "Epoch 14: loss = 1.59 | acc = 0.854 | val_loss = 1.597 |val_acc = 0.849\n",
      "Epoch 15: loss = 1.589 | acc = 0.855 | val_loss = 1.604 |val_acc = 0.843\n",
      "Epoch 16: loss = 1.591 | acc = 0.856 | val_loss = 1.601 |val_acc = 0.848\n",
      "Epoch 17: loss = 1.592 | acc = 0.857 | val_loss = 1.596 |val_acc = 0.851\n",
      "Epoch 18: loss = 1.591 | acc = 0.858 | val_loss = 1.604 |val_acc = 0.846\n",
      "Epoch 19: loss = 1.594 | acc = 0.857 | val_loss = 1.59 |val_acc = 0.856\n",
      "Epoch 20: loss = 1.591 | acc = 0.857 | val_loss = 1.602 |val_acc = 0.848\n",
      "Epoch 21: loss = 1.596 | acc = 0.859 | val_loss = 1.598 |val_acc = 0.85\n",
      "Epoch 22: loss = 1.593 | acc = 0.86 | val_loss = 1.593 |val_acc = 0.85\n",
      "Epoch 23: loss = 1.595 | acc = 0.861 | val_loss = 1.612 |val_acc = 0.851\n",
      "Epoch 24: loss = 1.594 | acc = 0.861 | val_loss = 1.613 |val_acc = 0.847\n",
      "Epoch 25: loss = 1.598 | acc = 0.861 | val_loss = 1.608 |val_acc = 0.856\n"
     ]
    }
   ],
   "source": [
    "m.train(batch_size=256,epochs=25,lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed8ba112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8458"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(x_test,y_test)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37765d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code of the backpropagation algorithm \n",
    "class NN:\n",
    "    def __init__(self, X,Y, L=1,N_l=256,epochs=10, lr=0.001):\n",
    "#     self.sizes = sizes\n",
    "#     self.epochs = epochs \n",
    "#     self.lr = lr\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.layer_sizes =np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.__init_weights()\n",
    "        self.__init_pre_gradient()\n",
    "        self.__init_learning_rate()\n",
    "        self.__init_pre_change_rate()\n",
    "        self.__init_gradient()\n",
    "        self.__init_change_rate()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.train_time,self.tot_time]\n",
    "        shuffle = np.random.permutation(self.n_samples)\n",
    "        self.X , self.X_val = np.split(self.X,[int(0.8*self.n_samples)])\n",
    "        self.Y , self.Y_val = np.split(self.Y,[int(0.8*self.n_samples)])\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "#     def __sigmoid(self,x):\n",
    "#     # VCompute the sigmoid\n",
    "#         return 1./(1.+np.exp(-x))\n",
    "\n",
    "    def _positive_sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def _negative_sigmoid(self,x):\n",
    "        # Cache exp so you won't have to calculate it twice\n",
    "        exp = np.exp(x)\n",
    "        return exp / (exp + 1)\n",
    "        \n",
    "    def __sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return self.sigmoid(x,False)*(1 - self.sigmoid(x,False))\n",
    "        positive = x >= 0\n",
    "        # Boolean array inversion is faster than another comparison\n",
    "        negative = ~positive\n",
    "\n",
    "        # empty contains juke hence will be faster to allocate than zeros\n",
    "        result = np.empty_like(x)\n",
    "        result[positive] = self._positive_sigmoid(x[positive])\n",
    "        result[negative] = self._negative_sigmoid(x[negative])\n",
    "\n",
    "        return result\n",
    "    def __softmax(self,x):\n",
    "        # Compute softmax along the rows of the input   \n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "\n",
    "    def __loss(self,y_pred,y):\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "\n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "\n",
    "    def __sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "\n",
    "#     def __init_prev_gradient(self):\n",
    "#         self.prev_gradient = list()\n",
    "#         for i in range(self.layer_sizes.shape[0]-1):\n",
    "#             self.prev_gradient.append(np.zeros(size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "\n",
    "    def __init_gradient(self):\n",
    "        self.gradient = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.gradient.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
    "\n",
    "    def __init_pre_gradient(self):\n",
    "        self.pre_gradient = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.pre_gradient.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
    "            \n",
    "    def __init_pre_change_rate(self):\n",
    "        self.pre_change_rate = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.pre_change_rate.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))  \n",
    "            \n",
    "    def __init_change_rate(self):\n",
    "        self.change_rate = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.change_rate.append(np.zeros((self.layer_sizes[i],self.layer_sizes[i+1])))  \n",
    "   \n",
    "    def __init_learning_rate(self):\n",
    "        self.learning_rate = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.learning_rate.append(np.full((self.layer_sizes[i],self.layer_sizes[i+1]),0.01))\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations \n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "        \n",
    "    def __feed_forward(self,batch):\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__sigmoid(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "        \n",
    "#         return curr_change_rate\n",
    "    def __back_prop(self,batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        delta_t = (self.__out - batch_y)*self.__sigmoid_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.gradient[-i] = (self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "#             self.change_rate[-i] = self.resilient(self.pre_gradient[-i],self.gradient[-i],self.learning_rate[-i],self.pre_change_rate[-i])\n",
    "            delta_t = self.__sigmoid_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-= self.gradient[-i]\n",
    "            \n",
    "    def predict(self,X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        X = X\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)         \n",
    "\n",
    "            \n",
    "    def __to_categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "\n",
    "    def train(self,batch_size=16,epochs=25,lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} |val_acc = {val_acc.round(3)}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67d7a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NN(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "398624ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 1.791 | acc = 0.596 | val_loss = 1.688 |val_acc = 0.724\n",
      "Epoch 2: loss = 1.669 | acc = 0.732 | val_loss = 1.655 |val_acc = 0.747\n",
      "Epoch 3: loss = 1.646 | acc = 0.749 | val_loss = 1.639 |val_acc = 0.755\n",
      "Epoch 4: loss = 1.632 | acc = 0.758 | val_loss = 1.629 |val_acc = 0.767\n",
      "Epoch 5: loss = 1.624 | acc = 0.768 | val_loss = 1.622 |val_acc = 0.767\n",
      "Epoch 6: loss = 1.617 | acc = 0.773 | val_loss = 1.617 |val_acc = 0.783\n",
      "Epoch 7: loss = 1.612 | acc = 0.78 | val_loss = 1.612 |val_acc = 0.774\n",
      "Epoch 8: loss = 1.607 | acc = 0.786 | val_loss = 1.608 |val_acc = 0.789\n",
      "Epoch 9: loss = 1.604 | acc = 0.792 | val_loss = 1.606 |val_acc = 0.789\n",
      "Epoch 10: loss = 1.601 | acc = 0.797 | val_loss = 1.603 |val_acc = 0.794\n",
      "Epoch 11: loss = 1.598 | acc = 0.801 | val_loss = 1.601 |val_acc = 0.801\n",
      "Epoch 12: loss = 1.595 | acc = 0.808 | val_loss = 1.598 |val_acc = 0.807\n",
      "Epoch 13: loss = 1.593 | acc = 0.813 | val_loss = 1.596 |val_acc = 0.803\n",
      "Epoch 14: loss = 1.591 | acc = 0.816 | val_loss = 1.596 |val_acc = 0.812\n",
      "Epoch 15: loss = 1.589 | acc = 0.821 | val_loss = 1.593 |val_acc = 0.818\n",
      "Epoch 16: loss = 1.587 | acc = 0.824 | val_loss = 1.592 |val_acc = 0.822\n",
      "Epoch 17: loss = 1.585 | acc = 0.826 | val_loss = 1.59 |val_acc = 0.824\n",
      "Epoch 18: loss = 1.584 | acc = 0.829 | val_loss = 1.589 |val_acc = 0.822\n",
      "Epoch 19: loss = 1.582 | acc = 0.831 | val_loss = 1.587 |val_acc = 0.826\n",
      "Epoch 20: loss = 1.581 | acc = 0.833 | val_loss = 1.586 |val_acc = 0.826\n",
      "Epoch 21: loss = 1.579 | acc = 0.835 | val_loss = 1.585 |val_acc = 0.829\n",
      "Epoch 22: loss = 1.578 | acc = 0.836 | val_loss = 1.584 |val_acc = 0.831\n",
      "Epoch 23: loss = 1.577 | acc = 0.838 | val_loss = 1.583 |val_acc = 0.83\n",
      "Epoch 24: loss = 1.575 | acc = 0.839 | val_loss = 1.582 |val_acc = 0.835\n",
      "Epoch 25: loss = 1.574 | acc = 0.84 | val_loss = 1.581 |val_acc = 0.835\n",
      "Epoch 26: loss = 1.573 | acc = 0.842 | val_loss = 1.58 |val_acc = 0.837\n",
      "Epoch 27: loss = 1.572 | acc = 0.842 | val_loss = 1.579 |val_acc = 0.837\n",
      "Epoch 28: loss = 1.571 | acc = 0.843 | val_loss = 1.578 |val_acc = 0.837\n",
      "Epoch 29: loss = 1.57 | acc = 0.844 | val_loss = 1.577 |val_acc = 0.839\n",
      "Epoch 30: loss = 1.569 | acc = 0.845 | val_loss = 1.577 |val_acc = 0.838\n",
      "Epoch 31: loss = 1.568 | acc = 0.845 | val_loss = 1.576 |val_acc = 0.838\n",
      "Epoch 32: loss = 1.567 | acc = 0.847 | val_loss = 1.575 |val_acc = 0.84\n",
      "Epoch 33: loss = 1.567 | acc = 0.847 | val_loss = 1.575 |val_acc = 0.84\n",
      "Epoch 34: loss = 1.566 | acc = 0.848 | val_loss = 1.574 |val_acc = 0.843\n",
      "Epoch 35: loss = 1.565 | acc = 0.848 | val_loss = 1.573 |val_acc = 0.844\n",
      "Epoch 36: loss = 1.565 | acc = 0.849 | val_loss = 1.574 |val_acc = 0.842\n",
      "Epoch 37: loss = 1.564 | acc = 0.85 | val_loss = 1.573 |val_acc = 0.843\n",
      "Epoch 38: loss = 1.563 | acc = 0.85 | val_loss = 1.572 |val_acc = 0.844\n",
      "Epoch 39: loss = 1.563 | acc = 0.85 | val_loss = 1.572 |val_acc = 0.846\n",
      "Epoch 40: loss = 1.562 | acc = 0.851 | val_loss = 1.571 |val_acc = 0.844\n",
      "Epoch 41: loss = 1.561 | acc = 0.852 | val_loss = 1.571 |val_acc = 0.845\n",
      "Epoch 42: loss = 1.561 | acc = 0.853 | val_loss = 1.57 |val_acc = 0.846\n",
      "Epoch 43: loss = 1.56 | acc = 0.853 | val_loss = 1.57 |val_acc = 0.848\n",
      "Epoch 44: loss = 1.56 | acc = 0.853 | val_loss = 1.57 |val_acc = 0.847\n",
      "Epoch 45: loss = 1.559 | acc = 0.853 | val_loss = 1.569 |val_acc = 0.846\n",
      "Epoch 46: loss = 1.559 | acc = 0.854 | val_loss = 1.569 |val_acc = 0.847\n",
      "Epoch 47: loss = 1.558 | acc = 0.855 | val_loss = 1.569 |val_acc = 0.848\n",
      "Epoch 48: loss = 1.558 | acc = 0.855 | val_loss = 1.568 |val_acc = 0.849\n",
      "Epoch 49: loss = 1.557 | acc = 0.856 | val_loss = 1.568 |val_acc = 0.849\n",
      "Epoch 50: loss = 1.557 | acc = 0.856 | val_loss = 1.568 |val_acc = 0.848\n"
     ]
    }
   ],
   "source": [
    "m.train(batch_size=256,epochs=50,lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27872330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8349"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2322847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Error: 0.15234150484410414, Accuracy: 16.928333333333335%\n",
      "Epoch 2/50, Error: 0.17816535077543438, Accuracy: 9.985%\n",
      "Epoch 3/50, Error: 0.09078365324355787, Accuracy: 25.426666666666662%\n",
      "Epoch 4/50, Error: 0.09977163674148777, Accuracy: 18.45%\n",
      "Epoch 5/50, Error: 0.09941350536574904, Accuracy: 20.171666666666667%\n",
      "Epoch 6/50, Error: 0.1027843102465564, Accuracy: 16.388333333333332%\n",
      "Epoch 7/50, Error: 0.11110224690641374, Accuracy: 4.951666666666667%\n",
      "Epoch 8/50, Error: 0.09205951503301177, Accuracy: 11.621666666666666%\n",
      "Epoch 9/50, Error: 0.09972641143821134, Accuracy: 22.220000000000002%\n",
      "Epoch 10/50, Error: 0.09594750042844677, Accuracy: 10.118333333333334%\n",
      "Epoch 11/50, Error: 0.10293909682850327, Accuracy: 17.015%\n",
      "Epoch 12/50, Error: 0.09435921030494773, Accuracy: 11.551666666666668%\n",
      "Epoch 13/50, Error: 0.1076283857032868, Accuracy: 18.736666666666665%\n",
      "Epoch 14/50, Error: 0.09162051161144597, Accuracy: 11.465%\n",
      "Epoch 15/50, Error: 0.12395151825753206, Accuracy: 19.98%\n",
      "Epoch 16/50, Error: 0.09128060319480497, Accuracy: 11.315%\n",
      "Epoch 17/50, Error: 0.1103659778264657, Accuracy: 16.331666666666667%\n",
      "Epoch 18/50, Error: 0.09260893007364815, Accuracy: 20.626666666666665%\n",
      "Epoch 19/50, Error: 0.10853104409516104, Accuracy: 19.895%\n",
      "Epoch 20/50, Error: 0.09151717754883895, Accuracy: 19.491666666666667%\n",
      "Epoch 21/50, Error: 0.10130540657207025, Accuracy: 19.645000000000003%\n",
      "Epoch 22/50, Error: 0.09501704386810618, Accuracy: 17.938333333333333%\n",
      "Epoch 23/50, Error: 0.10079877846671333, Accuracy: 16.919999999999998%\n",
      "Epoch 24/50, Error: 0.10213002058346543, Accuracy: 10.101666666666667%\n",
      "Epoch 25/50, Error: 0.09003607462039169, Accuracy: 21.726666666666667%\n",
      "Epoch 26/50, Error: 0.0943847675120825, Accuracy: 21.366666666666667%\n",
      "Epoch 27/50, Error: 0.10080589059760574, Accuracy: 28.073333333333334%\n",
      "Epoch 28/50, Error: 0.09765915598848968, Accuracy: 21.503333333333334%\n",
      "Epoch 29/50, Error: 0.09862906566106854, Accuracy: 29.396666666666665%\n",
      "Epoch 30/50, Error: 0.09495980999437958, Accuracy: 11.756666666666666%\n",
      "Epoch 31/50, Error: 0.09104485841479806, Accuracy: 27.416666666666668%\n",
      "Epoch 32/50, Error: 0.09780925948141048, Accuracy: 18.935%\n",
      "Epoch 33/50, Error: 0.09488248280616035, Accuracy: 20.441666666666666%\n",
      "Epoch 34/50, Error: 0.09792745060193228, Accuracy: 16.126666666666665%\n",
      "Epoch 35/50, Error: 0.09374508268509413, Accuracy: 18.076666666666664%\n",
      "Epoch 36/50, Error: 0.09748093760317543, Accuracy: 13.276666666666667%\n",
      "Epoch 37/50, Error: 0.09167137174517055, Accuracy: 23.581666666666667%\n",
      "Epoch 38/50, Error: 0.09785119528435798, Accuracy: 16.865%\n",
      "Epoch 39/50, Error: 0.09125802008096126, Accuracy: 27.561666666666667%\n",
      "Epoch 40/50, Error: 0.09853870230759179, Accuracy: 16.19666666666667%\n",
      "Epoch 41/50, Error: 0.08744313936619784, Accuracy: 28.923333333333336%\n",
      "Epoch 42/50, Error: 0.09930186085049447, Accuracy: 15.161666666666667%\n",
      "Epoch 43/50, Error: 0.08777641369656801, Accuracy: 29.54%\n",
      "Epoch 44/50, Error: 0.099039650934519, Accuracy: 15.076666666666666%\n",
      "Epoch 45/50, Error: 0.0892649116546569, Accuracy: 29.331666666666667%\n",
      "Epoch 46/50, Error: 0.09929855127336391, Accuracy: 14.625%\n",
      "Epoch 47/50, Error: 0.08866664608521882, Accuracy: 28.083333333333332%\n",
      "Epoch 48/50, Error: 0.10598210196952029, Accuracy: 10.885%\n",
      "Epoch 49/50, Error: 0.09592277576543062, Accuracy: 26.81833333333333%\n",
      "Epoch 50/50, Error: 0.09472042901690389, Accuracy: 15.233333333333333%\n",
      "Test Error: 0.0933014680451001, Test Accuracy: 28.389999999999997\n"
     ]
    }
   ],
   "source": [
    "# Levenberg Marquardt algorithm\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def convert(imgf, labelf, outf, n):\n",
    "    f = open(imgf, \"rb\")\n",
    "    o = open(outf, \"w\")\n",
    "    l = open(labelf, \"rb\")\n",
    "\n",
    "    f.read(16)\n",
    "    l.read(8)\n",
    "    images = []\n",
    "\n",
    "    for i in range(n):\n",
    "        image = [ord(l.read(1))]\n",
    "        for j in range(28 * 28):\n",
    "            image.append(ord(f.read(1)))\n",
    "        images.append(image)\n",
    "\n",
    "    for image in images:\n",
    "        o.write(\",\".join(str(pix) for pix in image) + \"\\n\")\n",
    "    f.close()\n",
    "    o.close()\n",
    "    l.close()\n",
    "\n",
    "\n",
    "mnist_train_x = \"train-images-idx3-ubyte\"\n",
    "mnist_train_y = \"train-labels-idx1-ubyte\"\n",
    "mnist_test_x = \"t10k-images-idx3-ubyte\"\n",
    "mnist_test_y = \"t10k-labels-idx1-ubyte\"\n",
    "\n",
    "convert(mnist_train_x, mnist_train_y, \"train.csv\", 60000)\n",
    "convert(mnist_test_x, mnist_test_y, \"test.csv\", 10000)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    encoded_labels = np.zeros((len(labels), len(unique_labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded_labels[i, label] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def accuracy_percentage(y_true, y_pred):\n",
    "    correct_predictions = np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1)\n",
    "    return np.mean(correct_predictions) * 100\n",
    "\n",
    "\n",
    "def clip_gradients(gradients, threshold):\n",
    "    clipped_gradients = []\n",
    "    for gradient in gradients:\n",
    "        clipped_gradients.append(np.clip(gradient, -threshold, threshold))\n",
    "    return clipped_gradients\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.biases_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.biases_output = np.zeros((1, output_size))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.biases_hidden\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        self.output = softmax(np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_output)\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, X, y, learning_rate, gradient_clip=None):\n",
    "        output_error = y - self.output\n",
    "        hidden_error = np.dot(output_error, self.weights_hidden_output.T) * sigmoid_derivative(self.hidden_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += learning_rate * np.dot(self.hidden_output.T, output_error)\n",
    "        self.biases_output += learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
    "        self.weights_input_hidden += learning_rate * np.dot(X.T, hidden_error)\n",
    "        self.biases_hidden += learning_rate * np.sum(hidden_error, axis=0, keepdims=True)\n",
    "\n",
    "        # Clip gradients if specified\n",
    "        if gradient_clip is not None:\n",
    "            gradients = [self.weights_input_hidden, self.biases_hidden,\n",
    "                         self.weights_hidden_output, self.biases_output]\n",
    "            clipped_gradients = clip_gradients(gradients, gradient_clip)\n",
    "            self.weights_input_hidden, self.biases_hidden, \\\n",
    "                self.weights_hidden_output, self.biases_output = clipped_gradients\n",
    "\n",
    "    def compute_jacobian(self, X):\n",
    "        # Forward pass to compute activations\n",
    "        self.forward_pass(X)\n",
    "\n",
    "        # Compute derivatives for the sigmoid function\n",
    "        sigmoid_deriv = sigmoid_derivative(self.hidden_output)\n",
    "\n",
    "        # Compute derivatives for the softmax function\n",
    "        softmax_deriv = np.apply_along_axis(self._softmax_derivative, 1, self.output)\n",
    "        # Compute the Jacobian for the hidden layer\n",
    "        jacobian_hidden = np.dot(self.weights_input_hidden, np.diag(sigmoid_deriv[0]))\n",
    "        \n",
    "        # Compute the Jacobian for the output layer\n",
    "        jacobian_output = np.dot(self.weights_hidden_output, np.diag(softmax_deriv[0]))\n",
    "\n",
    "        # Combine the two Jacobians\n",
    "        jacobian = np.dot(jacobian_hidden, jacobian_output)\n",
    "        return jacobian\n",
    "\n",
    "    def _softmax_derivative(self, softmax_output):\n",
    "        # Derivative of softmax is a bit complex since it depends on all other outputs\n",
    "        s = softmax_output.reshape(-1, 1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "    def train(self, X, y, epochs, learning_rate, gradient_clip=None):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward_pass(X)\n",
    "            self.backward_pass(X, y, learning_rate, gradient_clip)\n",
    "            error = mean_squared_error(y, output)\n",
    "            acc = accuracy_percentage(y, output)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Error: {error}, Accuracy: {acc}%\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward_pass(X)\n",
    "    \n",
    "# Load the training data\n",
    "train_file = open(\"train.csv\", \"r\")\n",
    "train_list = train_file.readlines()\n",
    "train_file.close()\n",
    "train_data = [list(map(int, record.split(','))) for record in train_list]\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_data[:, 1:] / 255.0  # Normalize pixel values to between 0 and 1\n",
    "y_train = one_hot_encode(train_data[:, 0])\n",
    "\n",
    "# Initialize and train the neural network using Levenberg-Marquardt with gradient clipping\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 256  # Increased hidden layer size\n",
    "output_size = 10\n",
    "\n",
    "neural_net = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "neural_net.train(X_train, y_train, epochs=50, learning_rate=0.001, gradient_clip=1.0)\n",
    "\n",
    "# Load the testing data\n",
    "test_file = open(\"test.csv\", 'r')\n",
    "test_list = test_file.readlines()\n",
    "test_file.close()\n",
    "\n",
    "test_data = [list(map(int, record.split(','))) for record in test_list]\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "# Extract features and labels\n",
    "X_test = test_data[:, 1:] / 255.0  # Normalize pixel values to between 0 and 1\n",
    "y_test = one_hot_encode(test_data[:, 0])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = neural_net.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_error = mean_squared_error(y_test, predictions)\n",
    "test_acc = accuracy_percentage(y_test, predictions)\n",
    "print(f'Test Error: {test_error}, Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397809f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
